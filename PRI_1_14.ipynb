{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is from group 14, composed by:\n",
    "\n",
    "- Ana Evans 86379\n",
    "- Artur Guimaraes 86389\n",
    "- Francisco Rosa 86417\n",
    "\n",
    "\n",
    "This Notebook showcases the functional part of the first delivery. In each section we  present the function and a set of outputs. After each funtion we will mention the structure and the meaning of each input and output. Alternatively, you can include a standard funtion signature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "hide_code": true
    }
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import nltk\n",
    "import spacy\n",
    "import whoosh\n",
    "import shutil\n",
    "import sklearn\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "from heapq import nlargest \n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from whoosh import index\n",
    "from whoosh import scoring\n",
    "from whoosh.qparser import *\n",
    "from whoosh.fields import *\n",
    "from sklearn.metrics import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "topics = {}\n",
    "judged_documents = {}\n",
    "index_id = 1\n",
    "# -----------------------------------------------------------------------\n",
    "# getTopics - Auxiliary function that gathers info on all topics\n",
    "#\n",
    "# Input: directory - Directory path for project materials\n",
    "# \n",
    "# Behaviour: Extracts topic info from '{directory}topics.txt' and updates\n",
    "# the global dictionary which stores topic info\n",
    "#\n",
    "# Output: None\n",
    "# -----------------------------------------------------------------------\n",
    "def getTopics(directory):\n",
    "    global topics\n",
    "    \n",
    "    topic_f = open('{}topics.txt'.format(directory), 'r')\n",
    "    parsed_file = BeautifulSoup(topic_f.read(), 'lxml')\n",
    "\n",
    "    topic_list = parsed_file.find_all('top')\n",
    "\n",
    "    for topic in topic_list:\n",
    "        split_topic = topic.getText().split('\\n')\n",
    "        split_topic = list(filter(lambda x: x!='', split_topic))\n",
    "\n",
    "        number = split_topic[0].split(' ')[2][1:]\n",
    "        title = processing(split_topic[1])\n",
    "        topics[int(number)] = re.sub(' +',' ',title)  \n",
    "    return\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# get_R_set - Auxiliary function that extracts the R set\n",
    "#\n",
    "# Input: directory - Directory path for project materials\n",
    "# \n",
    "# Behaviour: Extracts the triplet (Topic id, Document id, Feedback) for each entry in the \n",
    "# R set, present in '{directory}qrels_test.txt' (R-test) and '{directory}qrels_test.txt' (R-train)\n",
    "#\n",
    "# Output: [R-Test, R-Train], each being a list of triplet entries\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "def get_R_set(directory):\n",
    "    global judged_documents\n",
    "\n",
    "    r_test_f = open('{}qrels_test.txt'.format(directory), 'r')\n",
    "    r_train_f = open('{}qrels_train.txt'.format(directory), 'r')\n",
    "\n",
    "    r_test_lines = r_test_f.readlines()\n",
    "    r_train_lines = r_train_f.readlines()\n",
    "\n",
    "    r_test_lines = [r_test_lines, r_train_lines]\n",
    "    r_set = [{},{}]\n",
    "    \n",
    "    for i in range(2):\n",
    "        for line in r_test_lines[i]:\n",
    "            split_entry = line.split(' ')\n",
    "            topic_id = int(split_entry[0][1:])\n",
    "            doc_id = int(split_entry[1])\n",
    "\n",
    "            if doc_id not in judged_documents: \n",
    "                judged_documents[doc_id] = True\n",
    "\n",
    "            feedback = int(split_entry[2])\n",
    "            \n",
    "            if topic_id not in r_set[i]:\n",
    "                r_set[i][topic_id] = {}\n",
    "            r_set[i][topic_id][doc_id] = feedback\n",
    "\n",
    "    return r_set\n",
    "\n",
    "#--------------------------------------------------\n",
    "# get_xml_files_recursively - Auxiliary function to get_files_from_directory\n",
    "#\n",
    "# Input: path - The path to the parent directory or file from which to start our recursive function\n",
    "#               \n",
    "# Behaviour: Creates a list with the path to every file that's an hierarquical child of parent directory path,\n",
    "# recursively going through each child in Post-Order traversing\n",
    "#\n",
    "# Output: A List with the paths to each file child\n",
    "#--------------------------------------------------\n",
    "def get_xml_files_recursively(path):\n",
    "    global judged_documents\n",
    "\n",
    "    files_list = []\n",
    "    directory_list = os.listdir(path)\n",
    "    for f in directory_list:\n",
    "        n_path = '{}{}/'.format(path,f)\n",
    "        if os.path.isdir(n_path):\n",
    "            files_list.extend(get_xml_files_recursively(n_path))\n",
    "        else:\n",
    "            files_list.append(re.sub('//','/','{}/{}'.format(path,f)))\n",
    "    return files_list\n",
    "\n",
    "# -------------------------------------------------\n",
    "# get_files_from_directory - Recursively gets all files from directory or file path, parsing the files from xml to objects\n",
    "# and spliting them in D_Test and D_Train in the conditions specified by our project\n",
    "#\n",
    "# Input: path - The path to the parent directory or file from which to start our search\n",
    "#               \n",
    "# Behaviour: It starts by creating a list with the path to every file that's an hierarquical child of parent directory path,\n",
    "# recursively going through each child in Post-Order traversing. Afterwards it parses each and every file from xml to a runtime\n",
    "# object using the BeautifulSoup library. At last after having all files in object form it splits the dataset in D_Test and D_Train\n",
    "# sets, according to their identifier (D_Test -> identifier > 1996-09-30   D_Train -> identifier <= 1996-09-30)\n",
    "#\n",
    "# Output: A List with the Lists of file objects present in D_Test and D_Train\n",
    "# -------------------------------------------------\n",
    "def get_files_from_directory(path):\n",
    "    file_list = get_xml_files_recursively(path)\n",
    "\n",
    "    parsed_files_test = []\n",
    "    parsed_files_train = []\n",
    "\n",
    "    for f in file_list:\n",
    "        date_identifier = int(f.split('/')[2])\n",
    "\n",
    "        open_file = open(f, 'r')\n",
    "        parsed_file = BeautifulSoup(open_file.read(), 'lxml')\n",
    "        \n",
    "        if parsed_file.copyright != None:\n",
    "            parsed_file.copyright.decompose()\n",
    "\n",
    "        if parsed_file.codes != None:\n",
    "            parsed_file.codes.decompose()\n",
    "              \n",
    "        if date_identifier <= 19960930:\n",
    "            parsed_files_train += [parsed_file,]\n",
    "        else:\n",
    "            parsed_files_test += [parsed_file,]\n",
    "\n",
    "    return (parsed_files_test, parsed_files_train)\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "# processing - Processes text in String form\n",
    "#\n",
    "# Input: text - The text in String form to be processed\n",
    "#        **kwargs - Optional named arguments, with the following functionality (default values prefixed by *)\n",
    "#               lowercasing [*True | False]: Flag to perform Lowercasing \n",
    "#               punctuation [*True | False]: Flag to remove punction\n",
    "#               spellcheck [True | *False]: Flag to perform spell check using TextBlob\n",
    "#               stopwords [*True | False]: Flag to remove Stop Words \n",
    "#               simplication [*lemmatization | stemming | None]: Flag to perform Lemmatization or Stemming\n",
    "#               \n",
    "# Behaviour: Procceses the text in the input argument text as refered to by the arguments in **kwargs,\n",
    "# behaviour being completely dependent on them except for Tokenization which is always performed\n",
    "#\n",
    "# Output: A String with the processed text \n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "def processing(text, **kwargs):\n",
    "\n",
    "    p_text = text\n",
    "    # Lowercasing the entire string\n",
    "    if 'lowercasing' not in kwargs or kwargs['lowercasing']:\n",
    "        p_text = p_text.lower()\n",
    "\n",
    "    #Remove punctuation\n",
    "    if 'punctuation' not in kwargs or kwargs['punctuation']:\n",
    "        p_text = re.sub(\"[/-]\",\" \",p_text)\n",
    "        p_text = re.sub(\"[.,;:\\\"\\'!?`´()$£€]\",\"\",p_text)\n",
    "\n",
    "    # Spell Check\n",
    "    if \"spellcheck\" in kwargs and kwargs['spellcheck']:          \n",
    "        p_text = str(TextBlob(p_text).correct())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(p_text)\n",
    "    string_tokens = ''\n",
    "\n",
    "    # Spell Check correction\n",
    "    if \"spellcheck\" in kwargs and kwargs['spellcheck']:\n",
    "        n_tokens = []\n",
    "        for word in tokens:           \n",
    "            n_tokens += ' {}'.format(TextBlob(word).correct)\n",
    "\n",
    "    # Lemmatization\n",
    "    if 'simplification' not in kwargs or kwargs['simplification'] == 'lemmatization':\n",
    "        lemma = WordNetLemmatizer()\n",
    "\n",
    "        #Remove stopwords\n",
    "        if 'stopwords' not in kwargs or kwargs['stopwords']:\n",
    "            for word in tokens:\n",
    "                if word not in stopwords.words('English'):   \n",
    "                    string_tokens += ' {}'.format(lemma.lemmatize(word))\n",
    "        else: \n",
    "            for word in tokens: \n",
    "                string_tokens += ' {}'.format(lemma.lemmatize(word))\n",
    "\n",
    "    # Stemming\n",
    "    elif kwargs['simplification'] == 'stemming':\n",
    "        stemer = nltk.stem.snowball.EnglishStemmer()\n",
    "\n",
    "        #Remove stopwords\n",
    "        if 'stopwords' not in kwargs or kwargs['stopwords']:\n",
    "            for word in tokens:\n",
    "                if word not in stopwords.words('English'):   \n",
    "                    string_tokens += ' {}'.format(stemer.stem(word))\n",
    "        else: \n",
    "            for word in tokens: \n",
    "                string_tokens += ' {}'.format(stemer.stem(word))\n",
    "\n",
    "    # Case for no simplification\n",
    "    else:\n",
    "        for word in tokens: \n",
    "            string_tokens += ' {}'.format(word)   \n",
    "\n",
    "    # Removing the first whitespace in the output \n",
    "    return string_tokens[1:]\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "# boolean_query_aux - Auxiliary function to boolean_query that will check repeated ocurrences of documents\n",
    "#\n",
    "# Input: document_lists - A List of Lists in which each inner List has all documents in which the n-th term appeared\n",
    "#        k - The number of terms we are using\n",
    "#\n",
    "# Behaviour: The function starts by calculating our error margin, in other words the number of missmatches a document\n",
    "# can have before we stop considering it as relevant. This function composes a very simple algorithmn, where for each\n",
    "# document we find in a sublist (non repeated, we use the list 'seen' to check that) we check if it's contained within \n",
    "# all other sublists, until it's not contained in miss_m + 1 lists. When that's the case, the document is no longer \n",
    "# relevant and we move on to the next one, iterating upon all elements of all sublists. The Time Complexity of this \n",
    "# function is O(N^2) while the Space Complexity is O(N)\n",
    "#\n",
    "# Output: A List of all relevants docs that don't exceed miss_m missmatches\n",
    "# -------------------------------------------------------------------------------------------------------------------\n",
    "def boolean_query_aux(document_lists, k):\n",
    "    miss_m = round(0.2*k)\n",
    "    seen = []\n",
    "    result_docs = []\n",
    "\n",
    "    for term_docs in document_lists:\n",
    "        for doc in term_docs:\n",
    "            if doc not in seen:\n",
    "                chances = miss_m\n",
    "                flag = True\n",
    "                for doc_list in document_lists:\n",
    "                    if doc not in doc_list:\n",
    "                        if chances == 0:\n",
    "                            flag = False\n",
    "                            break\n",
    "                        chances -= 1\n",
    "                if flag:\n",
    "                    result_docs += [doc,]\n",
    "                seen += [doc, ]\n",
    "\n",
    "    result_docs.sort()\n",
    "    return result_docs\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "# cosine_scoring - Function that scores a document based on cosine similarity \n",
    "#\n",
    "# Input: searcher - The searcher associated with the index I\n",
    "#        all the other arguments are built-ins from FunctionWeighting() and old whoosh.scoring\n",
    "#        documentation\n",
    "#\n",
    "# Behaviour: Uses the tf-idf result from searcher.idf() and applies cosine similarity formula\n",
    "# to it\n",
    "#\n",
    "# Output: cosine similarity weight vector formula \n",
    "# ------------------------------------------------------------------------------------------\n",
    "def cosine_scoring(searcher, fieldnum, text, docnum, weight, QTF=1):\n",
    "    idf = searcher.idf(fieldnum, text)\n",
    "\n",
    "    DTW = (1.0 + math.log(weight)) * idf\n",
    "    QMF = 1.0\n",
    "    QTW = ((0.5 + (0.5 * QTF/ QMF))) * idf\n",
    "    return DTW * QTW\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# reciprocal_rank_fusion - Auxiliary function to calculate the RRF for the top-p documents\n",
    "# Uses the formula RBF_score(f) = sum (1 / (50 + rank_f))\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "def reciprocal_rank_fusion(p, ranking_lists):\n",
    "    document_ranks = {}\n",
    "\n",
    "    for rank_l in ranking_lists:\n",
    "        for i in range(len(rank_l )):\n",
    "            if rank_l[i][0] not in document_ranks:\n",
    "                document_ranks[rank_l[i][0]] = 0\n",
    "            document_ranks[rank_l[i][0]] += 1 / (50 + i+1)\n",
    "\n",
    "    p_highest = None\n",
    "\n",
    "    if p != None:\n",
    "        p_highest = nlargest(p, document_ranks, key=document_ranks.get)\n",
    "    else:\n",
    "        p_highest = nlargest(len(document_ranks), document_ranks, key=document_ranks.get)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for p in p_highest:\n",
    "        results += [[p, document_ranks[p]]]  \n",
    "\n",
    "    return results\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# find_R_test_labels - Function that finds the test labels for a given R_Set\n",
    "#\n",
    "# Input: R_test - The R_Test set \n",
    "#\n",
    "# Behaviour: Extrapolates the feedback from the R_Test set to an array\n",
    "#\n",
    "# Output: The R_Test set labels in np array form\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "def find_R_test_labels(R_test):\n",
    "    r_labels = {}\n",
    "    for doc in R_test:\n",
    "        r_labels[doc] = R_test[doc]\n",
    "\n",
    "    return r_labels\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# find_ranked_query_labels - Function that finds the test labels for given query_docs and r_labels\n",
    "#\n",
    "# Input: query_docs - The ranked query docs\n",
    "#        r_labels - the labels R_Test set produced \n",
    "#\n",
    "# Behaviour: Compares de R_Test set feedback with the ranked docs\n",
    "#\n",
    "# Output: The labels for the ranked query docs in np array form\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "def find_ranked_query_labels(query_docs, r_labels):\n",
    "    q_docs = np.array(query_docs)\n",
    "    q_docs = q_docs[:,0]\n",
    "\n",
    "    query_labels = []\n",
    "    result_labels = []\n",
    "\n",
    "    for doc in query_docs:\n",
    "        if doc[0] in r_labels:\n",
    "            query_labels += [[doc[0], 1], ]\n",
    "            result_labels += [[doc[0], r_labels[doc[0]]], ]\n",
    "    \n",
    "    for doc in r_labels:\n",
    "        if doc not in q_docs:\n",
    "            query_labels += [[doc, 0], ]\n",
    "            result_labels += [[doc, r_labels[doc]], ]\n",
    "    \n",
    "\n",
    "    return [np.array(query_labels), np.array(result_labels)]  \n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# find_boolean_query_labels - Function that finds the test labels for given query_docs and r_labels\n",
    "#\n",
    "# Input: query_docs - The query docs\n",
    "#        r_labels - the labels R_Test set produced \n",
    "#\n",
    "# Behaviour: Compares the R_Test set feedback with the ranked docs\n",
    "#\n",
    "# Output: The labels for the query docs in np array form\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "def find_boolean_query_labels(query_docs, r_labels):\n",
    "    query_labels = []\n",
    "    result_labels = []\n",
    "\n",
    "    for doc in r_labels:\n",
    "        if doc in query_docs:\n",
    "            query_labels += [[doc, 1], ]\n",
    "            result_labels += [[doc, r_labels[doc]]]\n",
    "        else:\n",
    "            query_labels += [[doc, 0], ]\n",
    "            result_labels += [[doc, r_labels[doc]]]\n",
    "\n",
    "    return [np.array(query_labels), np.array(result_labels)]   \n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# bpref - Function that runs the bpref evaluation metric\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "def bpref(sol_labels):\n",
    "    R = 0\n",
    "    N = 0\n",
    "    bpref = 0\n",
    "    n_count = 0\n",
    "    for label in sol_labels:\n",
    "        if label == 0:\n",
    "            N += 1\n",
    "        else:\n",
    "            R += 1\n",
    "\n",
    "    for label in sol_labels:\n",
    "        if label == 0:\n",
    "            n_count += 1\n",
    "        else:\n",
    "            bpref += (1 - n_count/(min(R,N)))\n",
    "\n",
    "    return (1 / R) * bpref\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# evaluate_ranked_query - Auxiliary function to calculate statistical data\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "def evaluate_ranked_query(topic, o_labels, sol_labels, **kwargs):\n",
    "    results = {}\n",
    "\n",
    "    results['accuracy'] = accuracy_score(sol_labels, o_labels)\n",
    "    results['precision-micro'] = precision_score(sol_labels, o_labels, average='micro', zero_division=1)\n",
    "    results['precision-macro'] = precision_score(sol_labels, o_labels, average='macro', zero_division=1)\n",
    "    results['recall-micro'] =  recall_score(sol_labels, o_labels, average='micro')\n",
    "    results['recall-macro'] =  recall_score(sol_labels, o_labels, average='macro')\n",
    "    results['f-beta-micro'] = fbeta_score(sol_labels, o_labels, average='micro', beta=0.5)\n",
    "    results['f-beta-macro'] = fbeta_score(sol_labels, o_labels, average='macro', beta=0.5)\n",
    "    results['MAP'] = average_precision_score(sol_labels, o_labels)\n",
    "    results['BPREF'] = bpref(sol_labels)\n",
    "\n",
    "    if 'curves' in kwargs and kwargs['curves']:\n",
    "        precision, recall, _ = precision_recall_curve(sol_labels, o_labels)\n",
    "        PrecisionRecallDisplay(precision=precision, recall=recall).plot()\n",
    "        plt.title('Precision Recall curve for Ranked topic {}'.format(topic))\n",
    "        plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# evaluate_boolean_query - Auxiliary function to calculate statistical data\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "def evaluate_boolean_query(topic, o_labels, sol_labels, **kwargs):\n",
    "    results = {}\n",
    "\n",
    "    results['accuracy'] = accuracy_score(sol_labels, o_labels)\n",
    "    results['precision-micro'] = precision_score(sol_labels, o_labels, average='micro', zero_division=1)\n",
    "    results['precision-macro'] = precision_score(sol_labels, o_labels, average='macro', zero_division=1)\n",
    "    results['recall-micro'] =  recall_score(sol_labels, o_labels, average='micro')\n",
    "    results['recall-macro'] =  recall_score(sol_labels, o_labels, average='macro')\n",
    "    results['f-beta-micro'] = fbeta_score(sol_labels, o_labels, average='micro', beta=0.5)\n",
    "    results['f-beta-macro'] = fbeta_score(sol_labels, o_labels, average='macro', beta=0.5)\n",
    "    results['MAP'] = average_precision_score(sol_labels, o_labels)\n",
    "\n",
    "    \n",
    "    if 'curves' in kwargs and kwargs['curves']:\n",
    "        precision, recall, _ = precision_recall_curve(sol_labels, o_labels)\n",
    "        PrecisionRecallDisplay(precision=precision, recall=recall).plot()\n",
    "        plt.title('Precision Recall curve for Boolean topic {}'.format(topic))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# display_results - Auxiliary function to display calculated statistical data\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "def display_results_per_q(q, results_ranked, results_boolean):\n",
    "    print(\"\\nResult for search on Topic {}\".format(q))\n",
    "    print(\"\\nRanked Search:\")\n",
    "    for p in results_ranked:\n",
    "        result_str= ''\n",
    "        for m in results_ranked[p]:\n",
    "            result_str += '{} = {}, '.format(m, round(results_ranked[p][m],4)) \n",
    "        print(\"For p={}: {}\".format(p, result_str[:-2]))\n",
    "\n",
    "    print(\"\\nBoolean Search:\")\n",
    "    for k in results_boolean:\n",
    "        result_str= ''\n",
    "        for m in results_boolean[k]:\n",
    "            result_str += '{} = {}, '.format(m, round(results_boolean[k][m],4)) \n",
    "        print(\"For k={}: {}\".format(k, result_str[:-2]))\n",
    "\n",
    "    return\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# overlapping_terms() - Function that finds the overlapping terms for a given k range\n",
    "#\n",
    "# Input: \n",
    "#\n",
    "# Behaviour: Queries the top terms for all k's in a given k range and checks them for overlap\n",
    "#\n",
    "# Output: Data about the overlaping terms\n",
    "# --------------------------------------------------------------------------------------------\n",
    "def overlapping_terms():\n",
    "    I = index.open_dir(\"index_judged_docs_dir\", indexname='index_judged_docs')\n",
    "    k_range = [2,3,5,7,10,15]\n",
    "\n",
    "    for k in k_range:\n",
    "        top_terms = {}\n",
    "        for q in range(101,201,1):\n",
    "            results = extract_topic_query(q, I, k)\n",
    "            for r in results:\n",
    "                if r not in top_terms:\n",
    "                    top_terms[r] = 0\n",
    "                top_terms[r] += 1\n",
    "\n",
    "        r_terms = 0\n",
    "        for term in top_terms:\n",
    "            if top_terms[term] > 1:\n",
    "                r_terms += 1\n",
    "        print(\"\\nNumber of overlapping terms: {}\".format(r_terms))\n",
    "        print(\"Percent of overlapping terms: {}%\".format(round(r_terms/len(top_terms)*100,3)))\n",
    "        print(top_terms)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section composes the enterity of our code that's not directly tied to the 5 main functions or testing on them, including every auxiliary function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(FileIndex(FileStorage('index1_dir'), 'index1'), 0.018958, 4096)\n"
     ]
    }
   ],
   "source": [
    "def indexing(D, **kwargs):\n",
    "    global index_id\n",
    "\n",
    "    start_time = time.time()\n",
    "    ind_name = 'index{}'.format(str(index_id))\n",
    "    ind_dir = '{}_dir'.format(ind_name)\n",
    "\n",
    "    if os.path.exists(ind_dir):\n",
    "        shutil.rmtree(ind_dir)\n",
    "        os.mkdir(ind_dir)\n",
    "    else:\n",
    "        os.mkdir(ind_dir)\n",
    "\n",
    "    schema = Schema(id= NUMERIC(stored=True), content= TEXT(stored=True))\n",
    "    ind = index.create_in(ind_dir, schema=schema, indexname=ind_name)\n",
    "    ind_writer = ind.writer()\n",
    "\n",
    "    if not index.exists_in(ind_dir, indexname=ind_name):\n",
    "        print(\"Error creating index\")\n",
    "        return\n",
    "\n",
    "    for doc in D:\n",
    "        item_id = doc.newsitem.get('itemid')\n",
    "        title = processing(re.sub('<[^<]+>', \"\", str(doc.title)), **kwargs)\n",
    "        dateline = processing(re.sub('<[^<]+>|\\w[0-9]+-[0-9]+-[0-9]+\\w', \"\", str(doc.dateline)), **kwargs)\n",
    "        text = processing(re.sub('<[^<]+>', \"\", str(doc.find_all('text')))[1:-1], **kwargs)\n",
    "        \n",
    "        result = nltk.word_tokenize('{} {} {}'.format(title, dateline, text))\n",
    "        ind_writer.add_document(id=item_id, content=result)\n",
    "\n",
    "    ind_writer.commit()\n",
    "    \n",
    "    time_required = round(time.time() - start_time, 6)\n",
    "    \n",
    "    space_required = os.path.getsize(ind_dir)\n",
    "\n",
    "    return (ind, time_required, space_required)\n",
    "\n",
    "# Change this to given rcv1 directory. We've removed the folders that are not relevant to the project.\n",
    "D_set = get_files_from_directory(\"../rcv1/19960820/\")\n",
    "I =  indexing(D_set[0])\n",
    "print(I)\n",
    "\n",
    "# Baseline index for experiments with entire D_set\n",
    "# I = index.open_dir(\"index_judged_docs_dir\", indexname='index_judged_docs') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**indexing(D, kwargs)** - Creates an index after processing all text on data set D\n",
    "\n",
    "**Input:** (D, kwargs)\n",
    "\n",
    "        D - The data set we will be building the index with \n",
    "        \n",
    "        kwargs - Optional named arguments for text preprocessing, with the following functionality (default values prefixed by *)\n",
    "               lowercasing [*True | False]: Flag to perform Lowercasing \n",
    "               punctuation [*True | False]: Flag to remove punction\n",
    "               spellcheck [True | *False]: Flag to perform spell check using TextBlob\n",
    "               stopwords [*True | False]: Flag to remove Stop Words \n",
    "               simplication [*lemmatization | stemming | None]: Flag to perform Lemmatization or Stemming\n",
    "               \n",
    " **Behaviour:** This function starts by creating the directory for our Index, after initializing our Schema fields. It then\n",
    " processes all documents on data set D and stores valuable information from them on the index (identifier, title, dateline and text).\n",
    " At last it commits the resulting processed documents to our index and calculates the total computational time the function used and the\n",
    " Disk space required to store the index.\n",
    "\n",
    " **Output:** A triplet tuple with the Inverted Index in object structure, the computational time for the function and \n",
    " the disk space required to store the Inverted Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['death', 'council', 'accident', 'yearly', 'people', 'simpson', 'result', 'mining', 'mine', 'half']\n"
     ]
    }
   ],
   "source": [
    "def extract_topic_query(q, I, k, **kwargs):\n",
    "    global topics \n",
    "    topic = topics[q]\n",
    "\n",
    "    topic_terms = []\n",
    "    weight_vector = None\n",
    "\n",
    "    # Chooses which score model to use from kwargs\n",
    "    if 'scoring' not in kwargs:\n",
    "        weight_vector = scoring.BM25F(B=0.75, content_B=1.0, K1=1.5)\n",
    "\n",
    "    elif kwargs['scoring'] == 'freq':\n",
    "        weight_vector = scoring.Frequency()\n",
    "\n",
    "    elif kwargs['scoring'] == 'tf-idf':\n",
    "        weight_vector = scoring.TF_IDF()\n",
    "\n",
    "    elif kwargs['scoring'] == 'dfree':\n",
    "        weight_vector = scoring.DFree()\n",
    "\n",
    "    elif kwargs['scoring'] == 'pl2':\n",
    "        C = 1.0 if 'C' not in kwargs else kwargs['C']\n",
    "\n",
    "        weight_vector = scoring.PL2(c=C)\n",
    "\n",
    "    elif kwargs['scoring'] == 'bm25':\n",
    "        b = 0.75 if 'B' not in kwargs else kwargs['B']\n",
    "        content_b = 1.0 if 'content_B' not in kwargs else kwargs['content_B']\n",
    "        k1 = 1.5 if 'K1' not in kwargs else kwargs['K1']\n",
    "\n",
    "        weight_vector = scoring.BM25F(B=b, content_B=content_b, K1=k1)    \n",
    "\n",
    "    with I.searcher(weighting=weight_vector) as searcher:\n",
    "        parser = QueryParser(\"content\", I.schema, group=OrGroup).parse(topic)\n",
    "        results = searcher.search(parser, limit=None)\n",
    "        res_list = [int(r.values()[1]) for r in results]\n",
    "\n",
    "        numbers_list = []\n",
    "        for i in res_list:\n",
    "            numbers_list += [searcher.document_number(id=i),]\n",
    "\n",
    "        topic_terms = searcher.key_terms(numbers_list, \"content\", numterms=k, normalize=True)\n",
    "      \n",
    "    result = []\n",
    "    for term in topic_terms:\n",
    "        result += [term[0], ]\n",
    "\n",
    "    return result\n",
    "\n",
    "# This material folder is kept within the project directory \n",
    "material_dic = 'material/'\n",
    "\n",
    "getTopics(material_dic)\n",
    "print(extract_topic_query(120,I,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **extract_topic_query(q,I,k,kwargs)** - Return the top-k informative terms from the topic q agains I using parameterizable scoring\n",
    "\n",
    " **Input:** (q,I,k,kwargs)\n",
    " \n",
    "        q - The identifier number of the topic we want to search about\n",
    " \n",
    "        I - The Index object in which we will perform our search\n",
    "        \n",
    "        k - The number of top-k terms to return\n",
    "        \n",
    "        kwargs - Optional named arguments to parameterize scoring, with the following functionality (default values prefixed by *)\n",
    "               scoring [freq | tf-idf | dfree | pl2 |*bm25] - Chooses the scoring model we will use to score our terms\n",
    "               C [float | *1.0] - Free parameter for the pl2 model\n",
    "               B [float | *0.75] - Free parameter for the BM25 model\n",
    "               content_B [float | *1.0] - Free parameter specific to the content field for the BM25 model\n",
    "               k1 [float | *1.5] - Free parameter for the BM25 model\n",
    "\n",
    " **Behaviour:** Extracting the relevant model information from kwargs, this function uses the index I present in its arguments \n",
    " to perform a scored search on the top-k informative terms for topic q. It does so by creating a QueryParser object to parse\n",
    " the entire lenght of terms from q we've stored in our global topics structure and by using searcher.key_terms() to return\n",
    " the top terms according to our scoring weight vector. \n",
    "\n",
    " **Output:** A List that contains the top k terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188920, 198038, 212755, 213354, 325202, 326277, 367280, 367978, 383110, 385019, 480287, 523104, 557332, 558329]\n"
     ]
    }
   ],
   "source": [
    "def boolean_query(q, k, I, **kwargs):\n",
    "    terms = extract_topic_query(q, I, k, **kwargs)\n",
    "\n",
    "    document_lists = []\n",
    "    with I.searcher() as searcher:\n",
    "        for term in terms:\n",
    "            parser = QueryParser(\"content\", I.schema, group=OrGroup).parse(term)\n",
    "            results = searcher.search(parser, limit=None)\n",
    "            term_list = [int(r.values()[1]) for r in results]\n",
    "            document_lists += [term_list,]\n",
    "            \n",
    "    return boolean_query_aux(document_lists, k)\n",
    "\n",
    "print(boolean_query(120,5,I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **boolean_query(q,k,I,kwargs)** - Function that will query all documents in index I and find those who contain\n",
    " all top k-terms relevant to topic q allowing up to round(0.2*k) missmatches \n",
    "\n",
    " **Input:** (q,k,I,kwargs)\n",
    " \n",
    "        q - The identifier number of the topic we want to search about\n",
    " \n",
    "        k - The number of top k-terms to check documents for\n",
    "        \n",
    "        I - The Index object in which we will perform our search\n",
    "        \n",
    "        kwargs - Optional arguments refer to extract_topic_query() and are described as follows:\n",
    "               scoring [freq | tf-idf | dfree | pl2 |*bm25] - Chooses the scoring model we will use to score our terms\n",
    "               C [float | *1.0] - Free parameter for the pl2 model\n",
    "               B [float | *0.75] - Free parameter for the BM25 model\n",
    "               content_B [float | *1.0] - Free parameter specific to the content field for the BM25 model\n",
    "               k1 [float | *1.5] - Free parameter for the BM25 model\n",
    "\n",
    " **Behaviour:** The function starts by running extract_topic_query to return top k-terms with which\n",
    " we will search for the relevant docs for topic q. Then we use the index I to perform a simple\n",
    " search on, parsing the result of our search per term to our auxiliary function. \n",
    "\n",
    " **Output:** A List of all relevants docs that don't exceed miss_m missmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(584459, 28.254201392850014), (278616, 28.192934774700113), (404182, 27.12153764290254), (792492, 25.461922516243582), (171226, 25.19692728128043), (204747, 23.034778800657648), (123337, 22.999708352384996), (378363, 22.930812645277832), (486353, 22.521280927841886), (568999, 21.639495077237466), (190362, 21.060485320547304), (136594, 20.389647463670762), (436113, 20.11444001536727), (520094, 19.202343229641027), (705685, 18.857761187469187), (435901, 18.681432936218037), (693723, 18.561516348232836), (107933, 18.371357184750785), (519806, 18.293917399506192), (522470, 18.293917399506192), (397346, 18.180028882713714), (387471, 18.148527604028523), (772032, 18.137292100900307), (348003, 18.109250909798007), (359847, 18.013298772517736), (105063, 17.858644194621604), (95224, 17.84700265832251), (277667, 17.692147598392992), (190374, 17.547728297565968), (705688, 17.45938080146687), (110616, 17.30748587669453), (774533, 17.2400351818085), (790186, 17.144101698753467), (101115, 17.09881125658505), (299826, 17.072526696693767), (527643, 16.945987234474348), (394211, 16.910129359377468), (677849, 16.85640835894701), (190335, 16.812802454313847), (574691, 16.613886566133772), (786167, 16.596243913365495), (161811, 16.58971517367405), (296342, 16.485253927492703), (107907, 16.479209961184218), (388430, 16.3832460644121), (745693, 16.30773446472824), (190377, 16.293551725525205), (803786, 16.254260230313438), (693235, 16.253616702319228), (241359, 16.157245168405144)]\n"
     ]
    }
   ],
   "source": [
    "def ranking(q, p, I, **kwargs):\n",
    "    global topics\n",
    "    topic = topics[q]\n",
    "\n",
    "    weight_vector = None\n",
    "    if 'ranking' not in kwargs:\n",
    "        weight_vector = scoring.BM25F(B=0.75, content_B=1.0, K1=1.5)\n",
    "\n",
    "    elif kwargs['ranking'] == 'cosine':\n",
    "        weight_vector = scoring.FunctionWeighting(cosine_scoring)\n",
    "\n",
    "    elif kwargs['ranking'] == 'tf-idf':\n",
    "        weight_vector = scoring.TF_IDF()\n",
    "\n",
    "    elif kwargs['ranking'] == 'bm25':\n",
    "        b = 0.75 if 'B' not in kwargs else kwargs['B']\n",
    "        content_b = 1.0 if 'content_B' not in kwargs else kwargs['content_B']\n",
    "        k1 = 1.5 if 'K1' not in kwargs else kwargs['K1']\n",
    "\n",
    "        weight_vector = scoring.BM25F(B=b, content_B=content_b, K1=k1)  \n",
    "\n",
    "    elif kwargs['ranking'] == 'RRF':\n",
    "        bm25_ranking_1 = ranking(q, p, I, ranking=\"bm25\")\n",
    "        bm25_ranking_2 = ranking(q, p, I, ranking=\"bm25\", b=0.5, content_b=1.25, k1=1.25)\n",
    "        bm25_ranking_3 = ranking(q, p, I, ranking=\"bm25\", b=0.5, content_b=1.5, k1=1.00)\n",
    "\n",
    "        return reciprocal_rank_fusion(p, [bm25_ranking_1, bm25_ranking_2, bm25_ranking_3])\n",
    "\n",
    "    with I.searcher(weighting=weight_vector) as searcher:\n",
    "        parser = QueryParser(\"content\", I.schema, group=OrGroup).parse(topic)\n",
    "        results = searcher.search(parser, limit=p)\n",
    "        \n",
    "        term_list = []\n",
    "\n",
    "        if p != None:\n",
    "            for i in range(p):\n",
    "                if i < len(results):\n",
    "                    term_list += [(int(results[i].values()[1]), results.score(i)), ]\n",
    "        else:\n",
    "            for i in range(len(results)):\n",
    "                term_list += [(int(results[i].values()[1]), results.score(i)), ]\n",
    "\n",
    "    return term_list\n",
    "\n",
    "print(ranking(120,50,I))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **ranking(q,p,I,kwargs)** - Function that will query all documents in index I and rank the top p ones\n",
    "\n",
    " **Input:** (q,p,I,kwargs)\n",
    " \n",
    "        q - The identifier number of the topic we want to search about \n",
    "        \n",
    "        p - The number of top ranked documents we will return\n",
    "        \n",
    "        I - The Index object in which we will perform our search\n",
    "        \n",
    "        kwargs - Optional named arguments to parameterize scoring, with the following functionality (default values prefixed by *)\n",
    "               ranking [cosine | RRF | tf-idf | *bm25] - Chooses the scoring model we will use to score our terms\n",
    "               B [float | *0.75] - Free parameter for the BM25 model\n",
    "               content_B [float | *1.0] - Free parameter specific to the content field for the BM25 model\n",
    "               k1 [float | *1.5] - Free parameter for the BM25 model\n",
    "\n",
    " **Behaviour:** The function uses the weight vector generated by its given scoring system to search and rank  \n",
    " the top-p documents in the index according to the full topic text.\n",
    "\n",
    " **Output:** A List of lists that contains pairs [document_id, score] in descending score ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result for search on Topic 120\n",
      "\n",
      "Ranked Search:\n",
      "For p=100: accuracy = 0.7205, precision-micro = 0.7205, precision-macro = 0.7857, recall-micro = 0.7205, recall-macro = 0.6414, f-beta-micro = 0.7205, f-beta-macro = 0.6898, MAP = 0.534, BPREF = 0.5999\n",
      "For p=200: accuracy = 0.7301, precision-micro = 0.7301, precision-macro = 0.7491, recall-micro = 0.7301, recall-macro = 0.6675, f-beta-micro = 0.7301, f-beta-macro = 0.7052, MAP = 0.5427, BPREF = 0.6068\n",
      "For p=300: accuracy = 0.6627, precision-micro = 0.6627, precision-macro = 0.6368, recall-micro = 0.6627, recall-macro = 0.6216, f-beta-micro = 0.6627, f-beta-macro = 0.6303, MAP = 0.4669, BPREF = 0.5417\n",
      "For p=400: accuracy = 0.559, precision-micro = 0.559, precision-macro = 0.5393, recall-micro = 0.559, recall-macro = 0.5404, f-beta-micro = 0.559, f-beta-macro = 0.5391, MAP = 0.4021, BPREF = 0.4543\n",
      "For p=500: accuracy = 0.5518, precision-micro = 0.5518, precision-macro = 0.5476, recall-micro = 0.5518, recall-macro = 0.5504, f-beta-micro = 0.5518, f-beta-macro = 0.5445, MAP = 0.4075, BPREF = 0.4545\n",
      "For p=None: accuracy = 0.4699, precision-micro = 0.4699, precision-macro = 0.5678, recall-micro = 0.4699, recall-macro = 0.5452, f-beta-micro = 0.4699, f-beta-macro = 0.4814, MAP = 0.4035, BPREF = 0.464\n",
      "\n",
      "Boolean Search:\n",
      "For k=1: accuracy = 0.5735, precision-micro = 0.5735, precision-macro = 0.5125, recall-micro = 0.5735, recall-macro = 0.5094, f-beta-micro = 0.5735, f-beta-macro = 0.5019, MAP = 0.3854\n",
      "For k=2: accuracy = 0.6241, precision-micro = 0.6241, precision-macro = 0.6864, recall-micro = 0.6241, recall-macro = 0.5075, f-beta-micro = 0.6241, f-beta-macro = 0.3798, MAP = 0.3877\n",
      "For k=4: accuracy = 0.6265, precision-micro = 0.6265, precision-macro = 0.8119, recall-micro = 0.6265, recall-macro = 0.5095, f-beta-micro = 0.6265, f-beta-macro = 0.3814, MAP = 0.3925\n",
      "For k=6: accuracy = 0.6193, precision-micro = 0.6193, precision-macro = 0.8096, recall-micro = 0.6193, recall-macro = 0.5, f-beta-micro = 0.6193, f-beta-macro = 0.3352, MAP = 0.3807\n",
      "For k=8: accuracy = 0.6193, precision-micro = 0.6193, precision-macro = 0.8096, recall-micro = 0.6193, recall-macro = 0.5, f-beta-micro = 0.6193, f-beta-macro = 0.3352, MAP = 0.3807\n",
      "For k=10: accuracy = 0.6193, precision-micro = 0.6193, precision-macro = 0.8096, recall-micro = 0.6193, recall-macro = 0.5, f-beta-micro = 0.6193, f-beta-macro = 0.3352, MAP = 0.3807\n"
     ]
    }
   ],
   "source": [
    "def evaluation(Q_test, R_test, D_test, **kwargs):\n",
    "    I = index.open_dir(\"index_judged_docs_dir\", indexname='index_judged_docs')\n",
    "    #I = indexing(D_test, **kwargs)[0]\n",
    "\n",
    "    results_ranked = {}\n",
    "    results_boolean = {}\n",
    "    k_range = [1,2,4,6,8,10]\n",
    "    p_range = [100,200,300,400,500, None]\n",
    "\n",
    "    if 'k_range' in kwargs:\n",
    "        k_range = kwargs['k_range']\n",
    "    if 'p_range' in kwargs:\n",
    "        p_range = kwargs['p_range']\n",
    "\n",
    "    for q in Q_test:\n",
    "        r_labels = find_R_test_labels(R_test[q])\n",
    "\n",
    "        for p in p_range:\n",
    "            score_docs = ranking(q, p, I, **kwargs)\n",
    "            ranked_labels = find_ranked_query_labels(score_docs, r_labels)\n",
    "\n",
    "            results_ranked[p] = evaluate_ranked_query(q, ranked_labels[0][:, 1],ranked_labels[1][:, 1], **kwargs)\n",
    "\n",
    "        for k in k_range:\n",
    "            boolean_docs = boolean_query(q, k, I, **kwargs)\n",
    "            query_labels = find_boolean_query_labels(boolean_docs, r_labels)\n",
    "\n",
    "            results_boolean[k] = evaluate_boolean_query(q, query_labels[0][:, 1], query_labels[1][:, 1], **kwargs)\n",
    "            \n",
    "        display_results_per_q(q, results_ranked, results_boolean)\n",
    "        \n",
    "    return\n",
    "\n",
    "Q_test = [120]\n",
    "R_set = get_R_set(material_dic)\n",
    "evaluation(Q_test, R_set[0], D_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **evaluation(Qtest ,Rtest ,Dtest ,args)** - Function that fully evaluates our IR model, providing full statiscal analysis for several\n",
    " p and k values across multiple ranges and topics\n",
    "\n",
    " **Input:** (Qtest ,Rtest ,Dtest ,args)\n",
    " \n",
    "        Q_test - The set of topics we will evaluate the perform of our IR model on\n",
    "        \n",
    "        R_test - The number of top ranked documents we will return\n",
    "        \n",
    "        D_test - The Index object in which we will perform our search\n",
    "        \n",
    "        kwargs - The additional args in this function also refer to the additional args in indexing(),\n",
    "        ranking() and boolean_query(), for which documentation is provided above. Other than that, we have:\n",
    "               k_range [list of ints | *[1,2,4,6,8,10, 15]] - List of k values our model will test\n",
    "               p_range [list of ints or None | *[100,200,300,400,500, None]] - List of p values our model will test\n",
    "               curves [True | *False] - Display the precision/recall curves\n",
    "\n",
    " **Behaviour:** The function provides full statistics for every topic in Q_test, using R_test and D_test\n",
    " to build an index. Then, for each p in p_range it will use ranking() to rank the top p documents\n",
    " and for each k in k_range it will use k to evaluate the relevant docs using boolean_query(). In the end,\n",
    " it uses retrival results to provide full statiscal analysis.\n",
    "\n",
    " **Output:** Full statistical analysis for the provided input args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['death', 'council', 'accident', 'yearly', 'people', 'simpson', 'result', 'mining', 'mine', 'half', 'directive', 'possibly', 'killed', 'agenda', 'proposal', 'may', 'police', 'dec', 'brussels', 'said']\n",
      "['china', 'nuclear', 'pakistan', 'beijing', 'chinese', 'hong', 'council', '0', 'kong', 'taiwan', 'missile', 'korea', 'india', 'power', '*', 'north', 'state', 'south', 'official', 'u']\n",
      "['disease', 'council', 'agenda', 'bse', 'directive', 'european', 'cow', 'brussels', 'possibly', 'proposal', 'drug', 'eu', 'beef', 'hold', 'mad', 'patient', 'cattle', 'health', 'luxembourg', 'human']\n",
      "['newspaper', 'percent', 'year', 'said', 'decline', 'labour', 'party', 'daily', '*', 'conservative', 'china', 'government', 'would', 'poll', 'election', 'last', 'time', 'report', 'month', 'minister']\n",
      "[(250177, 16.188240126228926), (608919, 15.351559299148075), (650471, 15.199293586677655), (558586, 15.138922185783136), (401374, 14.63466917650236), (254857, 14.281682874793805), (808812, 14.101009870589053), (531800, 13.95254293211418), (531809, 13.95254293211418), (440590, 13.827630439645652), (189269, 13.806904331276147), (156222, 13.682942440328118), (249245, 13.477899778049299), (427415, 13.477899778049299), (249237, 13.384657593057604), (439481, 13.3641408498686), (286633, 13.103408717441013), (249236, 13.023770193347588), (650567, 12.998294764540715), (301307, 12.836924674858638)]\n",
      "[(468893, 868.4150825728235), (91111, 467.5715662252119), (129265, 400.8435163476116), (558586, 149.54594775220218), (96883, 110.8814324826708), (138528, 67.92253178379983), (150474, 67.92253178379983), (153005, 67.92253178379983), (301307, 59.98446245312721), (123826, 58.22917483895254), (148881, 58.22917483895254), (94280, 57.75916140774674), (250177, 54.704202677881824), (249238, 52.030855952682515), (119847, 51.51825611878142), (249638, 48.997570688024794), (294631, 48.997570688024794), (431293, 48.997570688024794), (135162, 39.4813711684543), (572487, 36.957987769234435)]\n",
      "[[249245, 0.058823529411764705], [427415, 0.057692307692307696], [558586, 0.05660377358490566], [250177, 0.05555555555555555], [427365, 0.05454545454545454], [729520, 0.05357142857142857], [650567, 0.05263157894736842], [427364, 0.05172413793103448], [549597, 0.05084745762711865], [531800, 0.05], [531809, 0.04918032786885246], [804123, 0.04838709677419355], [301307, 0.047619047619047616], [677097, 0.046875], [98313, 0.046153846153846156], [321739, 0.045454545454545456], [249638, 0.04477611940298507], [294631, 0.044117647058823525], [431293, 0.043478260869565216], [608919, 0.04285714285714286]]\n",
      "\n",
      "Result for search on Topic 105\n",
      "\n",
      "Ranked Search:\n",
      "For p=100: accuracy = 0.8798, precision-micro = 0.8798, precision-macro = 0.8087, recall-micro = 0.8798, recall-macro = 0.8039, f-beta-micro = 0.8798, f-beta-macro = 0.8077, MAP = 0.5339, BPREF = 0.0584\n",
      "For p=200: accuracy = 0.9031, precision-micro = 0.9031, precision-macro = 0.8326, recall-micro = 0.9031, recall-macro = 0.9171, f-beta-micro = 0.9031, f-beta-macro = 0.8428, MAP = 0.6519, BPREF = 0.6748\n",
      "For p=300: accuracy = 0.8992, precision-micro = 0.8992, precision-macro = 0.8277, recall-micro = 0.8992, recall-macro = 0.9147, f-beta-micro = 0.8992, f-beta-macro = 0.8378, MAP = 0.6428, BPREF = 0.6748\n",
      "For p=400: accuracy = 0.8682, precision-micro = 0.8682, precision-macro = 0.7959, recall-micro = 0.8682, recall-macro = 0.9107, f-beta-micro = 0.8682, f-beta-macro = 0.8038, MAP = 0.5895, BPREF = 0.702\n",
      "For p=1000: accuracy = 0.7481, precision-micro = 0.7481, precision-macro = 0.7134, recall-micro = 0.7481, recall-macro = 0.8362, f-beta-micro = 0.7481, f-beta-macro = 0.7009, MAP = 0.4288, BPREF = 0.6976\n",
      "\n",
      "Boolean Search:\n",
      "For k=2: accuracy = 0.8062, precision-micro = 0.8062, precision-macro = 0.9031, recall-micro = 0.8062, recall-macro = 0.5, f-beta-micro = 0.8062, f-beta-macro = 0.4194, MAP = 0.1938\n",
      "For k=3: accuracy = 0.7442, precision-micro = 0.7442, precision-macro = 0.6424, recall-micro = 0.7442, recall-macro = 0.6894, f-beta-micro = 0.7442, f-beta-macro = 0.6446, MAP = 0.3144\n",
      "For k=4: accuracy = 0.8721, precision-micro = 0.8721, precision-macro = 0.871, recall-micro = 0.8721, recall-macro = 0.6928, f-beta-micro = 0.8721, f-beta-macro = 0.7985, MAP = 0.4641\n",
      "For k=5: accuracy = 0.8062, precision-micro = 0.8062, precision-macro = 0.9031, recall-micro = 0.8062, recall-macro = 0.5, f-beta-micro = 0.8062, f-beta-macro = 0.4194, MAP = 0.1938\n",
      "\n",
      "Result for search on Topic 120\n",
      "\n",
      "Ranked Search:\n",
      "For p=100: accuracy = 0.7205, precision-micro = 0.7205, precision-macro = 0.7857, recall-micro = 0.7205, recall-macro = 0.6414, f-beta-micro = 0.7205, f-beta-macro = 0.6898, MAP = 0.534, BPREF = 0.5999\n",
      "For p=200: accuracy = 0.7301, precision-micro = 0.7301, precision-macro = 0.7491, recall-micro = 0.7301, recall-macro = 0.6675, f-beta-micro = 0.7301, f-beta-macro = 0.7052, MAP = 0.5427, BPREF = 0.6068\n",
      "For p=300: accuracy = 0.6627, precision-micro = 0.6627, precision-macro = 0.6368, recall-micro = 0.6627, recall-macro = 0.6216, f-beta-micro = 0.6627, f-beta-macro = 0.6303, MAP = 0.4669, BPREF = 0.5417\n",
      "For p=400: accuracy = 0.559, precision-micro = 0.559, precision-macro = 0.5393, recall-micro = 0.559, recall-macro = 0.5404, f-beta-micro = 0.559, f-beta-macro = 0.5391, MAP = 0.4021, BPREF = 0.4543\n",
      "For p=1000: accuracy = 0.5759, precision-micro = 0.5759, precision-macro = 0.6133, recall-micro = 0.5759, recall-macro = 0.6125, f-beta-micro = 0.5759, f-beta-macro = 0.5889, MAP = 0.4456, BPREF = 0.4883\n",
      "\n",
      "Boolean Search:\n",
      "For k=2: accuracy = 0.6241, precision-micro = 0.6241, precision-macro = 0.6864, recall-micro = 0.6241, recall-macro = 0.5075, f-beta-micro = 0.6241, f-beta-macro = 0.3798, MAP = 0.3877\n",
      "For k=3: accuracy = 0.6627, precision-micro = 0.6627, precision-macro = 0.7363, recall-micro = 0.6627, recall-macro = 0.5631, f-beta-micro = 0.6627, f-beta-macro = 0.562, MAP = 0.4449\n",
      "For k=4: accuracy = 0.6265, precision-micro = 0.6265, precision-macro = 0.8119, recall-micro = 0.6265, recall-macro = 0.5095, f-beta-micro = 0.6265, f-beta-macro = 0.3814, MAP = 0.3925\n",
      "For k=5: accuracy = 0.6241, precision-micro = 0.6241, precision-macro = 0.8111, recall-micro = 0.6241, recall-macro = 0.5063, f-beta-micro = 0.6241, f-beta-macro = 0.3667, MAP = 0.3886\n",
      "\n",
      "Result for search on Topic 105\n",
      "\n",
      "Ranked Search:\n",
      "For p=500: accuracy = 0.8256, precision-micro = 0.8256, precision-macro = 0.7604, recall-micro = 0.8256, recall-macro = 0.8842, f-beta-micro = 0.8256, f-beta-macro = 0.7626, MAP = 0.5202, BPREF = 0.7\n",
      "For p=600: accuracy = 0.8101, precision-micro = 0.8101, precision-macro = 0.7495, recall-micro = 0.8101, recall-macro = 0.8746, f-beta-micro = 0.8101, f-beta-macro = 0.7492, MAP = 0.4989, BPREF = 0.6996\n",
      "For p=700: accuracy = 0.7868, precision-micro = 0.7868, precision-macro = 0.7346, recall-micro = 0.7868, recall-macro = 0.8602, f-beta-micro = 0.7868, f-beta-macro = 0.7301, MAP = 0.4701, BPREF = 0.6988\n",
      "\n",
      "Boolean Search:\n",
      "For k=6: accuracy = 0.8062, precision-micro = 0.8062, precision-macro = 0.9031, recall-micro = 0.8062, recall-macro = 0.5, f-beta-micro = 0.8062, f-beta-macro = 0.4194, MAP = 0.1938\n",
      "For k=7: accuracy = 0.8062, precision-micro = 0.8062, precision-macro = 0.9031, recall-micro = 0.8062, recall-macro = 0.5, f-beta-micro = 0.8062, f-beta-macro = 0.4194, MAP = 0.1938\n",
      "For k=8: accuracy = 0.8062, precision-micro = 0.8062, precision-macro = 0.9031, recall-micro = 0.8062, recall-macro = 0.5, f-beta-micro = 0.8062, f-beta-macro = 0.4194, MAP = 0.1938\n",
      "For k=9: accuracy = 0.8062, precision-micro = 0.8062, precision-macro = 0.9031, recall-micro = 0.8062, recall-macro = 0.5, f-beta-micro = 0.8062, f-beta-macro = 0.4194, MAP = 0.1938\n",
      "\n",
      "Result for search on Topic 120\n",
      "\n",
      "Ranked Search:\n",
      "For p=500: accuracy = 0.5518, precision-micro = 0.5518, precision-macro = 0.5476, recall-micro = 0.5518, recall-macro = 0.5504, f-beta-micro = 0.5518, f-beta-macro = 0.5445, MAP = 0.4075, BPREF = 0.4545\n",
      "For p=600: accuracy = 0.5446, precision-micro = 0.5446, precision-macro = 0.549, recall-micro = 0.5446, recall-macro = 0.5518, f-beta-micro = 0.5446, f-beta-macro = 0.5429, MAP = 0.4082, BPREF = 0.4481\n",
      "For p=700: accuracy = 0.5494, precision-micro = 0.5494, precision-macro = 0.5611, recall-micro = 0.5494, recall-macro = 0.5643, f-beta-micro = 0.5494, f-beta-macro = 0.5518, MAP = 0.4154, BPREF = 0.4631\n",
      "\n",
      "Boolean Search:\n",
      "For k=6: accuracy = 0.6193, precision-micro = 0.6193, precision-macro = 0.8096, recall-micro = 0.6193, recall-macro = 0.5, f-beta-micro = 0.6193, f-beta-macro = 0.3352, MAP = 0.3807\n",
      "For k=7: accuracy = 0.6193, precision-micro = 0.6193, precision-macro = 0.8096, recall-micro = 0.6193, recall-macro = 0.5, f-beta-micro = 0.6193, f-beta-macro = 0.3352, MAP = 0.3807\n",
      "For k=8: accuracy = 0.6193, precision-micro = 0.6193, precision-macro = 0.8096, recall-micro = 0.6193, recall-macro = 0.5, f-beta-micro = 0.6193, f-beta-macro = 0.3352, MAP = 0.3807\n",
      "For k=9: accuracy = 0.6193, precision-micro = 0.6193, precision-macro = 0.8096, recall-micro = 0.6193, recall-macro = 0.5, f-beta-micro = 0.6193, f-beta-macro = 0.3352, MAP = 0.3807\n"
     ]
    }
   ],
   "source": [
    "# A List of some examples of using optional args with our code\n",
    "\n",
    "print(extract_topic_query(120,I,20, scoring='freq'))\n",
    "print(extract_topic_query(121,I,20, scoring='tf-idf'))\n",
    "print(extract_topic_query(122,I,20, scoring='pl2'))\n",
    "print(extract_topic_query(123,I,20, scoring='bm25', B=0.25, content_B= 1.25, k1=1.25))\n",
    "\n",
    "print(ranking(150, 20, I, ranking='bm25', B=3, content_B= 0.25, k1=0.25))\n",
    "print(ranking(150, 20, I, ranking='bm25', B=0.25, content_B= 3, k1=2))\n",
    "print(ranking(150, 20, I, ranking='RRF'))\n",
    "\n",
    "Q_test = [105,120]\n",
    "evaluation(Q_test, R_set[0], D_set[0], ranking='RRF', scoring='tf-idf', k_range=[2,3,4,5], p_range=[100,200,300,400,1000])\n",
    "evaluation(Q_test, R_set[0], D_set[0], ranking='bm25', scoring='freq', k_range=[6,7,8,9], p_range=[500,600,700])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
